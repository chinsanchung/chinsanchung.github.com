---
title: '머신러닝 공부 - 차원 축소와 PCA'
layout: single
author_profile: false
read_time: false
comments: false
share: true
related: true
categories:
  - study
toc: true
toc_sticky: true
toc_labe: 목차
description: 비지도 학습의 기술인 차원 축소와 PCA 에 대해 공부한 내용을 정리합니다.
tags:
  - machine_learning
---

비지도 학습의 학습 방법 중에는 군집뿐만 아니라 차원 숙소 알고리즘도 있습니다. 차원 축소란 무엇인지 언급하고, 대표적인 차원 축소 알고리즘인 **주성분 분석**에 대해 정리하겠습니다.

## 차원 축소

### 차원 축소의 의미

실제 세계의 데이터는 수천 또는 수백만 개의 특서을 가지고 있습니다. 이런 특성은 훈련을 느리게 할 뿐만 아니라, 과대적합의 위험이 커져 좋은 솔루션을 찾기 어렵게 만드는 이른바 **차원의 저주**를 가져옵니다.

이러한 문제를 해결하려면 차원, 즉 특성의 개수를 줄여야 하는데 **차원 축소** 알고리즘은 데이터를 잘 나타내는 일부 특성을 선택해 차원을 줄이면서, 데이터를 최대한 보존하여 모델의 성능을 높이는 알고리즘입니다.

### 특성 선택과 특성 추출

특성 선택과 특성 추출을 언급하기에 앞서 우선 **잠재 특성**이 무엇인지를 설명한 후 다음 내용을 정리하겠습니다.

#### 0. 잠재 특성

잠재 특성은 데이터셋 실제로 있는 특성은 아니지만, 특성들을 압축했음에도 데이터를 설명할 수 있는 특성을 말합니다. 예를 들어 방의 개수, 화장실의 개수, 발코니의 유무를 "집의 크기" 라는 잠재 특성으로 압축할 수 있습니다.

#### 1. 특성 선택

특성 선택은 가장 관련성이 높은 특성의 하위 집합을 찾습니다. (예를 들어 부동산의 가치를 파악할 때 "그 지역의 범죄율"은 하위 집합으로 선택할 수 있는 특성입니다.) 특성 선택의 메소드들은 아래와 같습니다.

- 필터 메소드: 순위 또는 정렬 알고리즘으로 덜 중요한 특성을 필터링합니다. 일반적으로 전처리 단계로 적용하며, 데이터와 특성 간의 상관 관계를 결정하는 도구에는 Pearson 상관 관계, 선형 판별 분석(LDA), 분산 분석(ANOVA)가 있습니다.
- 래퍼(wrapper) 메소드: 모델 성능에 미치는 영향을 직접 테스트해서 특성을 선택합니다. 학습 알고리즘을 여러 번 호출하기 때문에 계산 비용이 많이 드는 방식입니다. 정방향 검색, 역방향 검색 및 재귀적 특성 제거 등 여러 방법이 있습니다.

#### 2. 특성 추출

특성 추출은 **잠재 특성**이라는 새로운 특성을 추출하거나, 구성하는 작업을 포함합니다. 특성 추출을 위한 방법으로는 독립 성분 분석(Independent Component Analysis), random Projection 이 있습니다.

특성 선택과 달리 특성 추출은 잠재 특성을 이용해서 여러 특성을 통합할 수 있기 때문에, 데이터를 삭제해서 정보를 잃는 것과 달리 정보를 유지할 수 있습니다.

## 주성분 분석(PCA)

주성분 분석은(PCA)을 간단히 말하자면 데이터의 있는 분산이 큰 방향을 찾는 것으로 이해할 수 있습니다. 여기서 분산은 데이터가 널리 퍼져있는 정도로, 분산이 큰 방향은 데이터를 잘 표현하는 벡터로 생각할 수 있습니다. (분산이 큰 방향이라는 것은 즉 분산을 최대한 보존하는 방향을 뜻하기도 합니다.) 이렇게 찾은 방향의 벡터를 **주성분**이라 부릅니다. 주성분 분석은 고차원 데이터셋의 시각화에 가장 널리 사용합니다.

### 주성분(Principal Component)

주성분은 원래 특성들의 선형 조합으로, 특성의 수를 줄여 차원을 축소하는 것이 목적입니다.

주성분의 개수는 원본 데이터셋에 있는 특성의 개수와 같습니다. 그리고 주성분으로 바꾼 데이터는 차원이 줄어듭니다. 하지만 주성분은 분산이 가장 큰 방향이기 때문에 이 데이터는 원본이 가지고 있는 특성을 가장 잘 나타내고 있습니다. 주성분이 여러 개일 경우, 생성한 주성분은 서로 직교하게 됩니다.

### 주성분 분석의 주요 특징

1. 각 성분이 설명하는 분산의 양을 고유값(eigenvalue)라 부릅니다.
2. 주성분의 각 성분은 가중치 벡터이며 고유 벡터(eigenvectors)라고 부릅니다. 이미지의 어떤 픽셀이 숫자를 구분하는 데 도움을 주는지를 이해하게 해줍니다.
3. 주성분 분석으로 특성을 줄인 데이터를 다른 학습 알고리즘에 재사용하여 성능을 높이거나 훈련 속도를 빠르게 할 수 있습니다.
4. 주성분은 원본 데이터에 있는 어떤 방향에 대응하는 여러 특성이 조합된 선형 조합이기 때문에 매우 복잡합니다. 따라서 주성분 분석은 그래프의 두 축을 해석하기 어렵다는 단점이 있습니다.

### 사이킷런에서의 주성분 분석

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()
X = StandardScaler().fit_transform(data)
pca = PCA(n_components=2)
pca.fit(X)
X_pca = pca.transform(X)
```

주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지를 기록한 **설명된 분산**을 `explained_variance_ratio_`으로 확인하실 수 있습니다. 예시로 결과가 `array([0.84, 0.14])`라고 나왔다면, 첫 번째 주성분이 분산의 84%를 표현하고, 두 번째 주성분은 14%를 표현한다는 뜻입니다.

```python
explained_variance_ratio = pca.explained_variance_ratio_
```

압축을 했던 데이터를 다시 복원하는 `inverse_transform()` 메소드도 있습니다. 압축을 한 만큼 원본으로 완벽하게 복원할 수는 없지만, 적은 특성으로도 최대한 원본과 비슷하게 복원합니다.

```python
X_recovered = pca.inverse_transform(X_reduced)
```

## 참고

[Intro to Machine Learning with TensorFlow](https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230)

[혼자 공부하는 머신러닝+딥러닝 - 6-3 주성분 분석](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=257932080)

[파이썬 라이브러리를 활용한 머신러닝(번역개정판) - 3장 비지도 학습과 데이터 전처리](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=186846299)

[핸즈온 머신러닝 - 1부 8장 차원 축소](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=237677114)
