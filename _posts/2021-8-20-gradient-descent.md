---
title: '머신러닝 공부 - 경사 하강법'
layout: single
author_profile: false
read_time: false
comments: false
share: true
related: true
categories:
  - study
toc: true
toc_sticky: true
toc_labe: 목차
description: 경사 하강법에 대해 공부한 내용을 정리합니다.
tags:
  - machine_learning
---

## 개요

경사 하강법 알고리즘에 대해 그동안 읽어왔던 교재를 기반으로 정리하고자 합니다. 언어는 파이썬, 주요 라이브러리는 numpy, pandas, scikit-learn 을 사용합니다.

## 경사 하강법이란?

경사 하강법(GD)은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘입니다. 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것입니다.[^intro] (비용 함수는 모델의 성능이 얼마나 나쁜지를 측정하는 함수입니다.)

경사 하강법에서 중요한 파라미터는 **학습률**입니다. 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸립니다. 한편 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라가게 될지도 모릅니다. 이는 알고리즘을 더 큰 값으로 발산하게 만들어 적절한 해법을 찾지 못하게 합니다.[^learn_rate]

경사 하강법을 사용할 떄는 반드시 모든 특성이 같은 스케일을 갖도록 만들어야 합니다. 그렇지 않으면 수렴하는 데 훨씬 오래 걸립니다[^scaler]

## 확률적 경사 하강법(Stochastic Gradient Descent)

확률적 경사 하강법은 기본적인 개념은 경사 하강법과 동일하지만, 데이터 세트에서 무작위로 하나의 샘플을 골라 가장 경사(기울기)가 가파른 것을 선택해 내려가면서, 특정 위치에 도달할 때까지 그 과정을 반복하는 것입니다. 훈련 세트를 한 번 모두 사용하는 과정을 **에포크**라고 부릅니다.[^epoch] 에포크를 사이킷런에서는 `max_iter`라고 불리며 하이퍼파라미터로써 원하는대로 수치를 지정할 수 있습니다. 무작위로 경사를 내려가는만큼, 학습률을 너무 크게 하지 않도록 주의해야 합니다.(한 번에 많이 내려가서 올바른 결과에 도달하지 않을 가능성이 생깁니다.)

매 반복에서 다뤄야 할 데이터가 매우 적기 때문에 한 번에 하나의 샘플을 처리하면 되므로 알고리즘이 확실히 훨씬 빠릅니다. 또한 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련시킬 수 있습니다. 반면 확률적이므로 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정합니다.[^stochastic_gd]

## 사이킷런에서의 확률적 경사 하강법

회귀 문제에서는 `SGDRegressor`, 분류 문제에서는 `SGDClassifier` 클래스로 선언할 수 있습니다. 사용하는 파라미터는 `loss`, `penalty`, `max_iter`, `learning_rate` 등이 있습니다.

### 손실 함수 loss

경사 하강법에서 내려가야 할 경사를 지칭하는 말입니다. 참고: 비용 함수는 손실 함수의 다른 말입니다. 엄밀히 말하면 손실 함수는 샘플 하나에 대한 손실을 정의하고 비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말합니다. 하지만 보통 이 둘을 엄격히 구분하지 않고 섞어서 사용합니다.[^loss]

학습률에 따라 조금씩 이동해야 한다는 말은 손실 함수의 선이 연속적이어야 한다는 뜻이기도 합니다. 연속적인 손실 함수를 위해 사용하는 방법 중 하나가 **로지스틱 손실 함수**입니다. **로지스틱 손실 함수**는 이진 분류에서 사용하며 로그를 이용해 예측을 수행합니다. 타깃이 1일 떄의 값을 -log(예측 확률), 0일 때의 값을 -log(1 - 예측 확률)으로 계산합니다.

또한 다중 분류에서의 손실 함수는 **크로스엔트로피 손실 함수**, 회귀에서는 **평균 절댓값 오차** 또는 **평균 제곱 오차**를 사용합니다.

## 미니배치 경사 하강법과 배치 경사 하강법

미니배치 경사 하강법은 데이터 세트에서 여러 개의 샘플(**미니배치**라고 부릅니다)을 꺼내서 경사를 이동하는 방법입니다.

배치 경사 하강법은 샘플 대신 전체 데이터로 경사를 이동합니다. 일부가 아닌 모든 데이터를 사용하는만큼 안정적이지만, 컴퓨터 자원의 소모가 큽니다.

이 방법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 사용해 그레이디언트를 계산한다는 사실입니다. 훈련 세트가 커지면 매우 느려지게 됩니다.[^batch_cons]

## 주석

[^intro] 핸즈온 머신러닝 2판. 4. 모델 훈련. 165쪽
[^learn_rate] 핸즈온 머신러닝 2판. 4. 모델 훈련. 166쪽
[^scaler] 핸즈온 머신러닝 2판. 4. 모델 훈련. 168쪽
[^epoch] 혼자 공부하는 머신러닝+딥러닝. 4.2 확률적 경사 하강법. 202쪽
[^stochastic_gd] 핸즈온 머신러닝 2판. 4. 모델 훈련. 172쪽
[^loss] 혼자 공부하는 머신러닝+딥러닝. 4.2 확률적 경사 하강법. 203쪽
[^batch_cons] 핸즈온 머신러닝 2판. 4. 모델 훈련. 172쪽
