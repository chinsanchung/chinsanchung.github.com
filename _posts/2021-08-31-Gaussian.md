---
title: '머신러닝 공부 - 가우시안 혼합 모델과 클러스터 검증'
layout: single
author_profile: false
read_time: false
comments: false
share: true
related: true
categories:
  - study
toc: true
toc_sticky: true
toc_labe: 목차
description: 가우시안 혼합 모델과 클러스터 검증에 대해 공부한 내용을 정리합니다.
tags:
  - machine_learning
---

비지도 학습 중에서 가우시안 혼합 모델 클러스터링, 그리고 클러스터를 검증하는 방법을 정리합니다.

## 가우시안 혼합 모델

### 개요

**가우시안 혼합 모델**(GMM)을 알려면 우선 가우시안 분포에 대해 이해해야 합니다.

> 가우시안 분포는 정규분포라고도 하며, 평균점을 중심으로 좌우 대칭인 종 모양을 나타내는 분포 곡선을 보입니다. 즉, 아래의 형태를 띄게 됩니다.

![가우시안 분포](https://www.scienceall.com/wp-content/uploads/2016/09/%EA%B0%80%EC%9A%B0%EC%8A%A4%EB%B6%84%ED%8F%AC.jpg)

**가우시안 혼합 모델**은 파라미터가 알려지지 않은 여러 개의 가우시안 분포를 혼합한 분포에서 샘플이 생성되었다고 가정하는 확률 모델입니다. 모든 샘플은 하나의 클러스터를 형성하는데, 샘플이 주어지면 가우시안 분포 중 하나에서 생성되었다는 것을 알지만, 어떤 분포에서 온 것인지, 또 이 분포의 파라미터는 무엇인지 알지 못합니다.

### 가우시안 분포를 그래프로 표현하기

1차원일 때의 그래프는 위의 가우시안 분포와 동일한 종 모양의 형태를 띕니다. 이번에는 2차원 상의 가우시안 분포 이미지입니다.

![2차원 가우시안 분포](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-31-Gaussian_multivariate.PNG?raw=true)

### 기댓값-최대화(EM) 알고리즘

가우시안 혼합 모델으로 클러스터링을 하기 위해선 기댓값-최대화( Expectation Maximization) 알고리즘을 사용합니다. 순서는:

1. 소프트 클러스터 할당으로 샘플을 클러스터에 할당합니다. 즉, 현재 클러스터 파라미터에 기반하여 각 클러스터에 속할 확률을 예측하여 할당합니다. 이를 **기댓값 단계**(Expactation)이라 부릅니다.
2. 1단계에서 계산한 확률을 이용해서, 가우시안을 위한 새로운 파라미터, 즉 새로운 평균과 분산을 구해 클러스터를 업데이트합니다. 이를 **최대화 단계**(Maximization)이라 부릅니다.
3. 모든 클러스터의 합을 로그로 변환한 값이 입력을 했던 종료 조건의 값보다 작아질 때 종료합니다. 그렇지 않으면 다시 1, 2단계를 반복합니다.

### 가우시안 혼합 모델의 장단점

장점

- 소프트 클러스터 할당을 사용하기에, 분류를 할 때 다양한 카테고리의 일부로 설정할 수 있습니다.
- 클러스터의 형태를 유연하게 설정할 수 있어서, 클러스터는 다른 클러스터를 안에 포함시킬 수 있습니다.

단점

- 기댓값-최대화 알고리즘을 수행하기 전, _가우시안 분포의 매개변수를 신중하게 선택해야 합니다._ 매개변수에 따라 기댓값-최대화 알고리즘의 결과에 큰 영향을 미치기 때문입니다.
- 그 결과가 국소 최적화(local optimum) 에 수렴할 가능성이 있습니다.
- 수렴하는 속도가 느립니다.

### 사이킷런에서 실행하기

[사이킷런 공식문서](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)에서 자세한 설명을 읽으실 수 있습니다.

```python
from sklearn import datasets, mixture

X = datasets.load_iris().data
# 가우시안 모델을 선언합니다. 클러스터의 개수를 3으로 합니다.
gmm = mixture.GaussianMixture(n_components=3)
gmm.fit(X)
# predict 로 각 점들이 속한 클러스터의 라벨을 저장한 리스트를 반환합니다.
pred_gmm = gmm.predict(X)
```

### 가우시안 혼합 모델으로 이상치 탐지하기

**이상치 탐지**는 보통 샘플과 많이 다른 샘플을 감지하는 작업으로, 가우시안 혼합 모델의 경우 밀도가 낮은 지역에 있는 모든 샘플을 이상치로 간주합니다. 그러기 위해서 밀도의 임곗값을 정헤야 합니다. (예츨 들어 False Positive 가 많다면 임곗값을 낮추고, 반대로 False Negative 가 많으면 임곗값을 높여 얻을 수 있는 샘플의 수를 조절합니다.)

```python
densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]
```

### 클러스터의 개수를 선택하기

이너셔 또는 실루엣 점수(아래 클러스터 검증에서 언급합니다.)로 적절한 클러스터의 개수를 선택하는 K-평균과 달리, 가우시안 혼합 모델에서는 BIC(Bayesian Information Criterion)이나 AIC(Akaike Information Criterion)같이 이론적 정보 기준을 최소화하는 모델을 찾습니다.

$$\text{BIC} = log(m)p - 2log(\hat{L})$$
$$\text{AIC} = 2p - 2log(\hat{L})$$

m 은 샘플의 개수, p 는 모델이 학습할 파라미터의 개수, $\hat{L}$ 은 모델의 가능도 함수(likelihood function)의 최댓값입니다.

클러스터가 많아 학습할 파라미터가 많은 모델에 벌칙을, 데이터를 잘 학습하는 모델에 보상을 합니다.

### 베이즈 가우시안 혼합 모델

불필요한 클러스터의 가중치를 0으로 만들어 최적의 클러스터를 찾아주는 모델입니다. 데이터에 대한 정보가 있다고 가정하여 최적의 클러스터 개수보다 크다고 믿는 값을 `n_components`로 지정합니다.

```python
from sklearn.mixture import BayesianGaussianMixture
bgm = BayesianGaussianMixture(n_components=10, n_init=10)
bgm.fit(X)
```

## 클러스터 검증

### 클러스터를 분석하는 절차

![검증 절차](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-31-Gaussian_analysis_process.PNG?raw=true)

### 클러스터 검증

클러스터링의 결과를 평가하는 과정입니다. 총 3개의 카테고리로 이뤄져 있습니다.

#### 외부 클러스터 검증(External cluster validation)

클러스터링한 결과를 레이블이 지정된 데이터셋(실제 결과)과 비교합니다. Adjusted Rand Index(ARI), F-measure 같은 방법으로 그 값이 높은 모델이 더 좋은 모델으로 평가합니다.

#### 내부 클러스터 검증(Internal cluster validation)

외부의 정보를 참조하지 않고, 클러스터링 과정에서의 내부 정보로 검증하는 방법입니다. 검증뿐만 아니라 클러스터의 개수, 적절한 클러스터링 알고리즘을 찾는 것에도 활용할 수 있습니다.

참고로 최적의 모델을 선정할 때 K-평균, Single Link, Complete Link, Ward, GMM 은 실루엣 계수로 성능을 비교할 수 있지만 DBSCAN 은 DBCV 라는 다른 방법을 사용해서 비교해야 합니다.

#### 상대적인 클러스터 검증(Relative cluster validation)

동일 알고리즘에 대해서, 다른 매개변수를 적용해서 모델을 검증합니다. 일반적으로 최적의 클러스터 개수를 찾을 때 사용합니다.

### 사이킷런으로 클러스터 검증하기

이번 예제에는 `adjusted_rand_score` 를 사용합니다. [사이킷런 공식 문서](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)에서 자세한 내용을 확인하실 수 있습니다.

```python
import seaborn as sns
iris = sns.load_dataset('iris')
# 우선 모델을 만들어 예측 결과를 변수에 저장합니다.
kmeans_iris = KMeans(n_clusters=3)
pred_iris = kmeans_iris.fit_predict(iris[:-1])

# adjusted_rand_score(실제 결과, 모델이 예측한 결과) 형식으로 입력합니다.
from sklearn.metrics import adjusted_rand_score
kmeans_score = adjusted_rand_score(iris['species'], pred_iris)
print(kmeans_score) # 0.73

# 이번에는 가우시안 혼합 모델의 점수를 얻어봅니다.
gmm_iris = GaussianMixture(n_components=3)
gmm_iris.fit(iris[:-1])
pred_gmm_iris = gmm.predict(iris[:-1])

gmm_score = adjusted_rand_score(iris['species'], pred_gmm_iris)
print(gmm_score) # 0.90
```

## 참고

[Intro to Machine Learning with TensorFlow](https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230)
[사이언스올](https://www.scienceall.com/%EA%B0%80%EC%9A%B0%EC%8A%A4-%EB%B6%84%ED%8F%ACgaussian-distribution-3/)
[핸즈온 머신러닝 - 1부 9장 비지도 학습](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=237677114)
[Cluster Validation Essentials](https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/)
