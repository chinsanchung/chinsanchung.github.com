---
title: 'Introduction to Machine Learning with TensorFlow - Finding Donors for CharityML'
layout: single
author_profile: false
read_time: false
comments: false
share: true
related: true
categories:
  - review
toc: true
toc_sticky: true
toc_labe: 목차
description: Introduction to Machine Learning with TensorFlow 의 첫 프로젝트 "Finding Donors for CharityML"를 설명합니다.
tags:
  - Udacity
  - nanodegree
---

유다시티의 **Introduction to Machine Learning with TensorFlow** 의 첫 번째 프로젝트 "Finding Donors for CharityML"를 진행하면서, 주어진 데이터를 어떻게 분석했는지 그리고 문제의 정답을 왜 이렇게 작성했는지를 정리하고자 합니다.

## 개요

### 프로젝트에 대한 설명

이 프로젝트의 목표는 "수입이 $50,000 보다 큰 사람은 자선단체에 기부를 할 것이다."라는 가정 하에, 45221명의 데이터를 훈련시켜서 수입이 50,000 달러 이상인 사람을 예측하는 것입니다. 데이터 컬럼에 대한 설명을 [깃허브 레퍼지토리](https://github.com/udacity/intro-to-ml-tensorflow/tree/master/projects/p1_charityml)에서 가져왔습니다.

**Features**

- `age`: Age
- `workclass`: Working Class (Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked)
- `education_level`: Level of Education (Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool)
- `education-num`: Number of educational years completed
- `marital-status`: Marital status (Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse)
- `occupation`: Work Occupation (Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces)
- `relationship`: Relationship Status (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried)
- `race`: Race (White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black)
- `sex`: Sex (Female, Male)
- `capital-gain`: Monetary Capital Gains
- `capital-loss`: Monetary Capital Losses
- `hours-per-week`: Average Hours Per Week Worked
- `native-country`: Native Country (United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands)

**Target Variable**

- `income`: Income Class (<=50K, >50K)

타깃(종속변수) 항목인 income 을 살펴보니, 이번 문제는 두 클래스 중 하나를 예측하는 이진 분류, 그리고 정답이 있는 데이터를 훈련시키는 지도 학습 알고리즘으로 작성해야 한다는 것을 알 수 있습니다.

## 데이터 준비하기

기계학습을 수행하기 전에 데이터 전처리를 통해 데이터의 품질을 높여 학습에 좋은 영향을 줄 수 있어야 합니다. 이 데이터 세트에서는 `capital-gain` 과 `capital-loss` 항목의 값들이 중간이 없이 좌우로 흩어져 있음을 유다시티 측에서 제시했습니다.

![skewed distributions](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-24-intro-ml-nd-01%20skewed_distributions.png?raw=true)

매우 큰 값과 매우 작은 값이 학습 성능에 부정적인 영향을 주지 않도록 로그 변환을 적용했습니다.

```python
skewed = ['capital-gain', 'capital-loss']
features_log_transformed = pd.DataFrame(data = features_raw)
features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))
```

![변환된 모습](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-24-intro-ml-nd-01%20log_transformed_distributions.png?raw=true)

또한, MinMaxScaler 를 이용해 다른 숫자형 특성들을 스케일링하여 값의 범위를 통일시킵니다.

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler() # default=(0, 1)
numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']

features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)
features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])
```

범주형 데이터를 예측에 활용하기 위해 숫자형 값으로 바꾸는 `one-hot-encoding`으로 범주형 클래스의 개수만큼 특성을 늘렸습니다.

```python
features_final = pd.get_dummies(features_log_minmax_transform)
income = income_raw.apply(lambda x: 0 if x == '<=50K' else 1)
```

마지막으로 훈련 데이터 세트, 테스트 데이터 세트로 분리합니다.

```python
from sklearn.model_selection import train_test_split

# Split the 'features' and 'income' data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_final,
                                                    income,
                                                    test_size = 0.2,
                                                    random_state = 0)
```

### 질문 1. Naive Predictor Performace

이번 계산의 목적은 기본 모델이 어떻게 생겼는지 알아보는 것입니다. 문제는 beta 가 0.5 인 F-score 를 계산해서 출력하는 것입니다.

#### 참고하기

- Precision

정확도(정밀도)는 데이터 세트에서 참이라고 예측한 값에 초점을 맞춥니다. 정확도 값을 기반으로 최적화를 하면, 거짓과 참의 예측을 비교해서 참을 제대로 예측하고 있는지를 확인합니다.

$$\text{precision} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Positive}}$$

- Recall

리콜은 데이터 세트에서 실제로 참인 값에 중점을 둡니다. 리콜 값을 기반으로 최적화를 하면, 실제 음수 값을 어떻게 처리하는지에 관계없이 참이라는 것을 잘 예측하고 있는지를 확인합니다.

$$\text{recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negative}}$$

- F-Beta Score

매트릭의 조합을 동시에 확인하기 위해 F 베타 점수(F1 점수를 자주 활용합니다), ROC, AUC 등 여러 방법이
있습니다. $\beta$ 매개변수는 정확도가 F 점수에 가중치를 부여하는 정도를 제어해서, 정확도와 리콜을 동시에 고려할 수 있습니다. 일반적인 값은 1으로, 정확도와 리콜 사이의 조화 평균을 찾을 수 있습니다.

$$F_{\beta} = (1 + \beta^2) * \frac{\text{precision} * \text{recall}}{(\beta^2 * \text{precision}) + \text{recall}}$$

```python
true_positive = np.sum(income)
false_positive = income.count() - true_positive
true_negative = 0
false_negative = 0

accuracy = true_positive / income.count()
recall = true_positive / (true_positive + false_negative)
precison = true_postive / (true_positive + false_positive)

beta = 0.5
beta_square = beta ** 2
fscore = (1 + beta_square) * (precision * recall) / ((beta_square * precision) + recall)

# Print the results
print("Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]".format(accuracy, fscore))
```

### 질문 2. Model Application

여러 지도 학습 모델 중에서 이번 예측에 사용할 3개의 모델을 정하고, 그 이유를 작성하는 것입니다. 질문의 순서는 아래와 같습니다.

- 언급한 모델에 대한 실제 사례를 설명하시오.
- 모델의 강점은? 언제 좋은 성능을 보이는가?
- 모델의 단점은? 언제 나쁜 성능을 보이는가?
- 이번에 조사할 데이터를 이해한다고 할 때, 언급한 모델을 좋은 후보자로 선정하게 만드는 것은?

1. 로지스틱 회귀

- 대상자의 금융 데이터를 토대로 신용도를 예측해서 잠재적 위험을 줄이는 데 활용합니다. 또는 소비재의 마케팅 전략을 수립할 떄 사용합니다.
- 훈련과 예측 속도가 빠르다는 것, 대용량의 데이터에서도 잘 작동한다는 점, 그리고 예측을 어떻게 수행한 것인지를 쉽게 이해할 수 있다는 것이 장점입니다.
- 특성이 너무 많을 경우, 과대적합이 발생할 가능성이 높습니다. 그리고 비선형 문제에 적용하기 어렵습니다.
- 비록 단순한 모델일지라도, 쉽고 빠르게 결과를 낼 수 있다는 점을 높게 평가했습니다. 또한 이번에 해결해야 할 문제가 이진 분류 문제이기 떄문에 이진 불류를 위한 대표적인 모델인 로지스틱 회귀가 적합하다고 생각했습니다.

2. 랜덤 포레스트

- 병을 진단하거나, 미래의 주식 가격을 예측하는 등 다양한 산업에서 사용하고 있습니다.
- 앙상블 학습은 정형화된 데이터에 좋은 결과를 냅니다. 여러 개의 의사결정나무 모델으로 결과를 도출해서 1개의 의사결정나무 모델보다 과대적합이 잘 발생하지 않습니다. 파라미터 튜닝에 크게 신경을 쓰지 않아도 좋은 성과를 냅니다.
- 다차원의 분산된 데이터에는 나쁜 성과를 냅니다. 그리고 선형 모델보다 메모리를 많이 사용하고 학습과 예측 속도가 느립니다.
- 기본적인 성능이 좋아 파라미터 튜닝에 대해 고민을 덜 수 있습니다. 또한 데이터의 규모와 독립 변수의 개수가 크지 않기 때문에, 훈련과 예측을 빠르게 수행할 수 있어 랜덤 포레스트를 선택했습니다.

3. 서포트 벡터 머신

- 이미지를 분류하는 모델을 통해 공장에서 생산하는 제품의 품질을 평가할 수 있습니다. 또한 다양한 형태의 데이터 세트에도 잘 작동하기 떄문에, 생물학이나 의약품 등 여러 산업에서 분류 문제를 해결하는 데 사용합니다.
- 저차원 또는 다차원 데이터에서 효과적입니다. 또한 샘플의 개수보다 더 높은 차원일 때 효과적입니다.
- 좋은 모델을 만드려면 파라미터 튜닝과 전처리를 신경써야 합니다. 모델을 이해하기 쉽지 않고, 비전문가에게 예측의 과정을 설명하기 어렵습니다.
- 원-핫 인코딩으로 특성의 개수를 103개로 늘렸는데, 이는 다차원 데이터에서 효과적인 서포트 벡터 머신의 성능을 높여줄 것이라 기대해서 이 모델을 선택했습니다.

## 훈련과 예측 파이프라인 구축하기

훈련과 예측, 그리고 평가를 위한 함수를 만들어 재사용성을 높입니다. 사용하는 매개변수는 아래와 같습니다.

- learner: 훈련과 예측을 수행할 알고리즘입니다.
- sample_size: 훈련 세트에서 몇 개의 샘플을 사용할 것인지 정하는 정수입니다.
- X_train: 훈련 세트의 특성들입니다.
- y_train: 훈련 세트의 종속변수를 모았습니다.
- X_test: 테스트 세트의 특성들입니다.
- y_test: 테스트 세트의 종속변수를 모았습니다.

```python
from sklearn.metrics import fbeta_score, accuracy_score

def train_predict(learner, sample_size, X_train, y_train, X_test, y_test):
  results = {}

  start = time()
  leaner.fit(X_train[:sample_size], y_train[:sample_isize])
  end = time()

  results['train_time'] = end - start

  start = time()
  predictions_test =  learner.predict(X_test)
  ## predictions_train 은 평가를 위해 사용합니다.
  predictions_train = learner.predict(X_train[:300])
  end = time()

  results['pred_time'] = end - start

  results['acc_train'] = accuracy_score(y_train[:300], predictions_train)
  results['acc_test'] = accuracy_score(predictions_test, y_test)
  beta = 1
  results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=beta)
  results['f_test'] = fbeta_score(y_test, predictions_test, beta=beta

  print('{} trained on {} samples.'.format(learner.__class__.name__, sample_size))

  return results
```

## 실행: 초기 모델 평가하기

로지스틱 회귀, 랜덤 포레스트, 그리고 서포트 벡터 머신 3개의 모델을 `train_predict()` 으로 훈련, 예측, 그리고 평가를 합니다. 편의를 위해 동일한 결과를 얻도록 `random_state` 를 설정합니다.

```python
clf_A = LogisticRegression(random_state=0)
clf_B = RandomForestClassifier(random_state=0)
clf_C = SVC(random_state=0)
## 데이터에서 순서대로 100%, 10%, 1% 만큼 개수를 지정합니다.
samples_100 = len(y_train)
samples_10 = int(samples_100 / 10)
samples_1 = int(samples_100 / 100)

results = {}
for clf in [clf_a, clf_B, clf_C]:
  clf_name = clf.__class__.__name__
  results[clf_name][i] = \
  train_predict(clf, samples, X_train, y_train, X_test, y_test)
## Udacity 측에서 제공한 matplotlib 시각화 함수 evaluate 로 그래프를 그립니다.
vs.evaluate(results, accuracy, fscore)
```

![3개의 모델 성능 지표](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-24-intro-ml-nd-01%20three_models_evaulation.png?raw=true)

모델을 작성할 때 `random_state`외에 다른 파라미터를 입력하지 않은 이유는, **질문 3. 최고의 모델 선택하기**에서 선정한 모델으로 파라미터 튜닝을 진행하여 비교를 하기 위해서입니다.

### 질문 3. 최고의 모델 선택하기

3개의 모델의 성능 결과를 확인하고, 그 중에서 가장 좋은 성능을 내는 모델을 선택하고 그 이유를 작성하는 문제입니다. 성능을 확인하는 지표는 시간, 정확도, F-score 입니다.

그래프에서는 훈련 서브셋, 테스트 서브셋으로 구분해서 결과를 보여주는데, 저는 테스트 서브셋의 결과를, 그리고 x 축은 100% (모든 서브셋으로 훈련했다는 뜻입니다.)을 기준으로 결정했습니다.

- F-score 에서는 세 모델 모두 근소한 차이를 보이며 랜덤 포레스트, 서포트 벡터 머신, 로지스틱 회귀 순으로 점수가 높습니다.
- 예측/훈련 시간을 기준으로는 서포트 벡터 머신이 상대적으로 가장 오랜 시간이 걸렸습니다. 로지스틱 회귀와 랜덤 포레스트는 거의 2초 이내의 시간이 걸렸습니다.
- 이번 데이터와 알고리즘과의 적합성을 볼 떄, 정형 데이터를 처리하는 데 가장 성능이 좋은 알고리즘은 앙상블 학습이며, 그 중 랜덤 포레스트는 앙상블 학습의 대표주자 중 하나입니다.

F-score, 시간, 그리고 데이터와 알고리즘과의 적합성을 고려했을 때, 랜덤 포레스트를 최고의 모델로 선정했습니다.

### 질문 4. 선택한 모델을 문외한에게 가르친다고 가정하고 설명하기

이번 문제는 모델의 학습 방법, 예측 방법 등 모델에 대한 설명을 전문 용어 없이 설명하는 것입니다.

저는 우선 의사결정나무 모델의 개념을 간략히 설명한 후, 랜덤포레스트가 무엇인지 그리고 어떤 과정으로 예측을 수행하는지를 3단계로 구분해서 작성했습니다. 마지막으로 왜 랜덤 포레스트를 선택해야 하는지 그 장점에 대해 간략히 설명했습니다.

## 모델 튜닝

이번에는 모델의 파라미터를 튜닝합니다. 사용자가 직접 지정해야만 하는 파라미터를 **하이퍼파라미터**라고 부르는데, 최적의 하이퍼파라미터를 입력해서 모델의 성능을 올리는 것이 목적입니다.

사용자가 집적 입력할 수 있지만, 사이킷런의 `GridSearchCV`를 사용하면 하이퍼파라미터 탐색과 교차 검증을 자동으로 수행해줍니다.(교차 검증이란 훈련 세트의 일부를 나눠 모델을 평가하는 과정을 여러 번 반복하는 것입니다.)

```python
clf = RandomForestClassifier(random_state=0)
parameters = { 'n_estimators' : np.arange(100, 210, 10), 'max_depth' : [6, 8, 10, 12], 'min_samples_leaf' : [8, 12, 18],
              'min_samples_split' : [8, 16, 20]}

## 테스트 세트에서 교차 검증한 모델의 성능을 평가할 전략을 설정합니다.
scorer = make_scorer(fbeta_score)

grid_obj = GridSearchCV(clf, parameters, n_jobs=-1, scoring=scorer)
grid_fit = grid_obj.fit(X_train, y_train)
best_clf = grid_fit.best_estimator_

predictions = (clf.fit(X_train, y_train)).predict(X_test)
best_predictions = best_clf.predict(X_test)

print("Unoptimized model\n------")
print("Accuracy score on testing data: {:.4f}".format(accuracy_score(y_test, predictions)))
print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta = 0.5)))
print("\nOptimized Model\n------")
print("Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))
print("Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5)))

'''
# 실행 결과
Unoptimized model
------
Accuracy score on testing data: 0.8405
F-score on testing data: 0.6769

Optimized Model
------
Final accuracy score on the testing data: 0.8426
Final F-score on the testing data: 0.7013
'''
```

### 질문 5. 마지막 모델 평가하기

처음에 실행했던 랜덤 포레스트 모델 `clf_B`와 하이퍼파라미터 튜닝으로 최적화한 모델을 비교합니다.

- 최적화한 모델의 정확도는 0.8426, F-score 는 0.7013 이 나왔습니다.
- 최적화한 모델의 F-score 가 처음 모델보다 0.0244점 더 높습니다.
- 질문 1에서 구한 Naive Predictor 의 정확도는 0.2478이었고 F-score는 0.6223이었습니다. 따라서 랜덤 포레스트는 Naive Predictor 보다 정확도에서 0.5948점, F 점수에서 0.0790점 더 높은 점수를 얻었습니다.

## 특성 중요도(Feature Importance)

가장 예측에 도움이 되는 특성이 무엇인지를 선별하는 것이 왜 중요한지 Udacity 측에서 설명한 것을 번역했습니다.

> 우리가 연구하는 인구 조사 데이터같은 데이터 셋으로 지도 학습을 수행하면서, 중요한 업무는 어떤 특성이 가장 예측력이 높은지를 결정하는 것입니다. 몇 개의 중요한 특성과 타깃 레이블 사이의 연관성에 집중하는 것으로 현상에 대한 이해를 단순화합니다. 이 작업은 항상 유용한 작업입니다.

랜덤 포레스트 모델에는 `feature_importance_` 속성이 있어서 특성의 중요도를 파악할 수 있습니다. 이를 통해 인구 조사 데이터 세트에서 가장 중요한 특성 5개를 살펴봅니다.

### 질문 6. 특성의 연관성 관찰하기

처음에 인구 조사 데이터를 탐색하면서, 타깃과 가장 높은 연관성이 있는 5개의 특성이 무엇인지, 그리고 중요성의 순서를 정하는 문제입니다. 저는 occupation, age, marital-status, relationship 그리고 capital-gain 순으로 중요한 특성이라고 판단했습니다.

#### 선정 이유

- relationship, occupation, marital-status 을 Y축을 income 으로 히스토그램을 그렸을 때, 대다수는 둘 다 수치가 미미했지만 일부 클래스의 경우 예측에 활용이 가능한 수준으로 분포되어 있습니다. 또한 seaborn 의 pairplot 으로 capital-gain 과 income 을 비교하면, ">50K"인지 아닌지의 경계가 뚜렷합니다.
- age 는 일반적으로 고령층이 젊은 층에 비해 동종업계 경험이 많고 직위가 높기 때문에 소득이 더 높다고 가정했기 떄문에 선정했습니다.

#### 중요성의 판단 기준

- 전통적으로 직업에 따라 기본적인 소득이 다르기 때문에 occupation 을 1위로, 일하는 업종에 대한 경력에 따른 소득의 차이를 생각해 age 를 2위로 정했습니다.
- 기혼자의 소득이 독신에 비해서 상대적으로 높을 가능성을 고려해서 martial-status 와 relationship 을 각각 3위, 4위로 지정했습니다.
- capital-gain 을 마지막으로 정한 이유는, 자본이 많더라도 소득이 그에 비례하지 않을 가능성도 있음을 고려했습니다. 예를 들어 주택 담보 대출을 받아 집을 구입한 사람이라면 소득이 50,000 달러 이하더라도 높은 capital-gain 수치를 가졌을 것이라 추측할 수 있습니다.

### 질문 7. 특성 중요도 추출하기

이번에는 GridSearchCV 로 얻은 최적의 파라미터로 모델을 만들어서 특성을 비교해봤습니다.

```python
# 그리드 서치로 얻은 최적의 파라미터 값들입니다.
model = RandomForestClassifier(max_depth=6, min_samples_leaf=8, min_samples_split=8, random_state=0)
model.fit(X_train, y_train)

importances = model.feature_importances_
# Udacity 측에서 제공한 그래프 출력 함수입니다.
vs.feature_plot(importances, X_train, y_train)
```

![특성 중요도](https://github.com/chinsanchung/chinsanchung.github.com/blob/master/assets/images/2021-08-24-intro-ml-nd-01%20five_features.png?raw=true)

이번 질문은 `model.feature_importances_`가 제시한 특성 5개와 질문 6에서 제가 제시한 특성 5개를 비교하는 것입니다.

- `model.feature_importances_`의 특성들은 질문 6에서 예측한 5개의 특성과 어떻게 비교됩니까?

martial-status, capital-gain, relationship, age 네 항목이 일치함을 확인할 수 있었습니다.

- 만약 두 예측이 비슷했다면, 위의 시각화를 본 후의 당신의 생각은?

martial-status, relationship, 그리고 age 가 중요한 특성이라고 가정한 것은 관습적인 추측이었는데, 실제로 일치함을 확인함으로써 인구 조사 데이터가 사회적인 관습을 어느 정도 반영하고 있다고 생각하게 됐습니다.

- 만약 두 예측이 많이 달랐다면, `model.feature_importances_`의 특성들이 더 관련성이 있는 이유는?

occupation 은 중요도가 5위에 들지 않는 값이었고, 대신 education-num 이 중요한 특성이었습니다.
그래프를 확인한 후 두 특성을 비교했더니 그 이유를 알 수 있었습니다.

```python
# 수입이 50000 달러를 초과하는 직업의 비율
(data.loc[data['income'] == '>50K']['occupation'].value_counts()
 / data['occupation'].value_counts()) * 100
'''
 Adm-clerical         13.646209
 Armed-Forces         28.571429
 Craft-repair         22.508306
 Exec-managerial      47.911096
 Farming-fishing      11.621622
 Handlers-cleaners     6.598240
 Machine-op-inspct    12.289562
 Other-service         4.076539
 Priv-house-serv       1.293103
 Prof-specialty       45.006658
 Protective-serv      31.454918
 Sales                26.904586
 Tech-support         28.943662
 Transport-moving     20.639033
Name: occupation, dtype: float64
'''
```

```python
# 수입이 50000 달러를 초과하는 교육을 받은 기간(연 단위)의 비율
(data.loc[data['income'] == '>50K']['education-num'].value_counts()
 / data['education-num'].value_counts()) * 100
'''
1.0      1.388889
2.0      3.603604
3.0      4.899777
4.0      6.682868
5.0      5.621302
6.0      6.704824
7.0      5.497221
8.0      7.452340
9.0     16.343097
10.0    20.103041
11.0    25.727412
12.0    26.410086
13.0    41.981506
14.0    55.409706
15.0    75.414013
16.0    73.345588
Name: education-num, dtype: float64
'''
```

수치를 비교했을 때 occupation 와 달리 education-num 은 숫자가 증가할수록 income 이 '>50K'인 확률과 확연하게 양의 상관관계를 보입니다. 이를 통해 occupation 대신 education-num 을 넣은 `model.feature_importances_`이 정답이었음을 알 수 있었습니다.

## 선택한 특성 5개로 훈련하기

`model.feature_importances_`로 선정한 5개의 특성만으로 훈련과 예측을 수행합니다. 전체 특성으로 했을 떄와 일부 특성으로 훈련했을 때의 성능을 비교했습니다.

```python
# 데이터의 컬럼을 5개로 줄이기
X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]
X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]

clf = (clone(best_clf)).fit(X_train_reduced, y_train)
reduced_predictions = clf.predict(X_test_reduced)

print("Final Model trained on full data\n------")
print("Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))
print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5)))
print("\nFinal Model trained on reduced data\n------")
print("Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, reduced_predictions)))
print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))

'''
Final Model trained on full data
------
Accuracy on testing data: 0.8426
F-score on testing data: 0.7013

Final Model trained on reduced data
------
Accuracy on testing data: 0.8434
F-score on testing data: 0.6992
'''
```

### 질문 8. 일부 특성 선택의 효과

위의 결과를 비교하면서, 만약 훈련 시간을 주요 요인으로 고려한다면 축소한 데이터를 훈련 세트로 사용하는 것을 고려하는가 묻는 문제입니다.

정확도와 F-score 를 비교했을 때 둘의 차이는 소수점 세 자리일 정도로 차이가 크지 않았습니다. 하지만 특성의 개수가 103개에서 5개로 줄어들면서 훈련과 예측 속도가 빨라졌습니다. 따라서 훈련 시간을 고려한다면 축소한 데이터를 고려할 것입니다. 왜냐면 시간을 큰 폭으로 절약하면서도 점수에 큰 영향을 주지 않기 때문입니다.

## 프로젝트를 마치며

지도 학습의 알고리즘을 하나씩 수행하고 점수를 구해 그래프로 비교할 수 있어서 모델의 성능을 보다 객관적으로 파악할 수 있었습니다.

한 가지 아쉬운 점은 x탐색적 자료 분석(EDA) 를 수행할 때 놓친 부분이 있어서 스스로 예측한 특성 중요도와 GridSearchCV 가 수행한 특성 중요도에서 차이점이 있었던 것입니다. 특히 'education-num' 항목의 경우 `value_counts()`로 비교하면 'education-num' 항목과 "income == '>50k'"이 양의 상관관계를 띄는 것을 바로 알 수 있었는데 놓쳐서 아쉬웠습니다. 앞으로 pandas, numpy, matplotlib 그리고 seaborn 을 더 학습해서 이해도 높은 EDA 를 도출하고자 합니다.
